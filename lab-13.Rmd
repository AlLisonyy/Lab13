---
title: "Lab 13 - Colonizing Mars"
author: "Yanying Li"
date: "05012025"
output: github_document
---

### Load packages and data

```{r load-packages, message=FALSE}
library(tidyverse) 
if (!require("ggplot2")) install.packages("ggplot2")
library(ggplot2)

if (!require("MASS")) install.packages("MASS")
library(MASS)

# Install and load the tidyverse package
if (!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)

library(psych)

```

### Exercise 1.0

```{r}
set.seed(123)
age <- rnorm(100, mean = 30, sd = 5)
df_colonists <- data.frame(
  id = 1:100, 
  age = age
) 

##Visualization:
ggplot(df_colonists, aes(x = age)) +
  geom_histogram(binwidth = 2, fill = "skyblue", color = "pink") +
  labs(title = "Age Distribution of Mars Colonists",
       x = "Age",
       y = "Number of Colonists") 
```


### Exercise 1.2

The three sets seem to have relatively normal distributions, with centers around age 30 and ranging from 15 to 45. Most of the ages are between 25 to 35. It seems like seeds controls the general distribution of the generated random samples, such as the mean, sd, and total numbers. 

I wonder if you might want us to draw the three visualization. It is a way to practice having three sets of seeds and generate distribution, which might be fun. 

### Exercise 1.3

```{r generate role varaibles}
set.seed(1)
df_colonists$role <- rep(c("engineer", "scientist", "medic"),
  times = c(33, 33, 33),
  length.out = 100
)
```

I used this one because I think it helps with equal distribution, and I would have a consistent and repeating pattern to make sure the roles are (almost) equally generated. Although there is one more engineer, I think this is the best way to have almost equal numbers of roles.

### Exercise 1.4

```{r}
set.seed(100)

df_colonists$marsgar <- rnorm(100, mean = 50, sd = 5)

ggplot(df_colonists, aes(
    x = age,
    y = marsgar
  ) )+
  geom_point()+
  geom_smooth(method=lm, color = "pink",se = FALSE)+
  labs(
      x = "Age",
      y = "MARSGAR",
      title = "Scatterplot of Age and Health"
  )
```

I think for learning about seed(), you can ask about using the same set seed for the graph first and then using a different one as the seed() used for age. I think this helps you illustrate the point that why no correlation is observed here!

### Exercise 2.1

```{r techn skills}

set.seed(1235)

df_colonists$technical_skills <- 2 * df_colonists$age + rnorm(100, mean = 0, sd = 1)

##visualization
##I do not see this as a suggestion, but I really like how your graph has the function as well, so i asked GPT to help me generate it. I am wondering if you would want students to have this as well since it is easier to see what is the intercept and coefficient. 

# the linear model
technical_model <- lm(technical_skills ~ age, data = df_colonists)
#Extract coefficients
intercept <- round(coef(technical_model)[1], 2)
slope <- round(coef(technical_model)[2], 2)

#Create equation text
eqn <- paste0("y = ", intercept, 
              ifelse(slope >= 0, " + ", " - "), 
              abs(slope), "x")

ggplot(df_colonists, aes(x = age, y = technical_skills)) +
  geom_point(color = "pink", alpha = 0.6) +
  geom_smooth(method = "lm", se = TRUE, color = "lightblue") +
  geom_label(aes(x = 22, y = max(technical_skills) - 5, label = eqn),
             fill = "white", color = "black",
             label.size = 0.5, label.padding = unit(0.2, "lines"))+
  labs(title = "Relationship Between Age and Technical Skills",
       x = "Age",
       y = "Technical Skills")
```


### Exercise 2.2

```{r}
## In my understanding, eginner might have the highest problem solving skills because they are often solving complex technical issues. After that might be scientist since they also mostly working with exploring and investigating problems and questions. Medics might have the least problem-solving ability since they focused more on curing and treating people. 
set.seed(132)

df_colonists$problem_solving[df_colonists$role == "engineer"] <-
  pmin(rnorm(sum(df_colonists$role == "engineer"), mean = 90, sd = 5), 100)

df_colonists$problem_solving[df_colonists$role == "scientist"] <-
  pmin(rnorm(sum(df_colonists$role == "scientist"), mean = 70, sd = 5), 100)

df_colonists$problem_solving[df_colonists$role == "medic"] <-
  pmin(rnorm(sum(df_colonists$role == "medic"), mean = 50, sd = 5), 100)
#problem_solving = role + noise
# Define base scores for each role
ggplot(df_colonists, aes(x = problem_solving, fill = role)) +
  geom_density(alpha = 0.8) +
  scale_fill_manual(values=c("lightpink", "lightblue","lightyellow"))+
  labs(title = "Distribution of Problem Solving by Role",
       x = "Problem Solving Score",
       y = "Density") 
```

I really like this exercise so I do not have much suggestions! I did add one thing to make the scores above 100 got cut because that will not make sense. 

### Exercise 3.1

MISSING? I did not see where is 3.1!

### Exercise 3.2

```{r Simulate resilience and agreeableness using mvrnorm}
# Define mean and covariance matrix
mean_traits <- c(50, 50)
cov_matrix <- matrix(c(100, 50, 50, 100), ncol = 2)

# Generate correlated data
set.seed(321)
traits_data <- mvrnorm(n = 100, mu = mean_traits, Sigma = cov_matrix, empirical = FALSE)

## Plot
traits_df <- as.data.frame(traits_data)
colnames(traits_df) <- c("Resilience", "Agreeableness")
ggplot(traits_df, aes(x = Resilience, y = Agreeableness)) +
  geom_point(color = "pink", alpha = 0.7) +
  geom_smooth(method = "lm", se = TRUE, color = "purple") +
  labs(
    title = "Scatter Plot of Resilience and Agreeableness",
    x = "Resilience",
    y = "Agreeableness"
  )
```


### Exercise 3.3

```{r}
#sample size:

seed <- 125
set.seed(seed)

n_colonist <- 100
#big five list
bigfive <- c("EX", "ES", "AG", "CO", "OP")
#M and SD, since they are going to be living on Mars, they might be having higher conscientiousness cores, lower neuroticism, and maybe slighly higher openness and agreeableness (to avoid potential conflicts with others during this process should be important). 
means_bigfive <- c(EX = 50, ES = 80, AG = 60, CO = 75, OP = 55)
sds_bigfive <- c(EX = 5, ES = 8, AG = 6, CO = 7, OP = 5)

cor_matrix_bigfive <- matrix(
  c(
1.0000, 0.2599, 0.1972, 0.1860, 0.2949,
0.2599, 1.0000, 0.1576, 0.2306, 0.0720,
0.1972, 0.1576, 1.0000, 0.2866, 0.1951,
0.1860, 0.2306, 0.2866, 1.0000, 0.1574,
0.2949, 0.0720, 0.1951, 0.1574, 1.0000
  ),
  nrow = 5, ncol = 5, byrow = TRUE,
  dimnames = list(
c("EX", "ES", "AG", "CO", "OP"),
c("EX", "ES", "AG", "CO", "OP")
  )
)

cov_bigfive <- cor2cov(cor_matrix_bigfive, sds_bigfive)

bigfive_data <- mvrnorm(n = 100, mu = means_bigfive, Sigma = cov_bigfive, empirical = FALSE)
bigfive_df <- as.data.frame(bigfive_data)
colnames(bigfive_df) <- c("EX", "ES", "AG", "CO", "OP")

##This is the part I started to get confused, so I looked at your answers on the codes
bigfive_data <- cbind.data.frame(
  colonist_id = 1:n_colonist, # add colonist_id
  seed = seed, # add seed
  EX = bigfive_data[, 1],
  ES = bigfive_data[, 2],
  AG = bigfive_data[, 3],
  CO = bigfive_data[, 4],
  OP = bigfive_data[, 5]
)

# Print the first few rows of the simulated data

print(head(bigfive_data))

summary_stats_mean <- bigfive_data %>%
  dplyr::select(-colonist_id, -seed) %>%
  summarize(across(everything(), list(mean = mean))) %>%
  rbind(means_bigfive) # compare with population parameters from mean_traits

## This one just did not work for me, it kept saying: `.fns` must be a function, a formula, or a list of functions/formulas. for the across(). I decided not to include this one just for now. But I am wondering what has happened here...
#summary_stats_sd <- bigfive_data %>%
  #dplyr::select(-colonist_id, -seed) %>%
  #summarize(across(everything(), list(sd = sd))) %>%
  #rbind(sds_bigfive)

# compare with population parameters from mean_traits and sd_traits
#summary_stats <- cbind(summary_stats_mean, summary_stats_sd)

#
summary_stats_cor <- bigfive_data %>%
  dplyr::select(-colonist_id, -seed) %>%
  cor() # compare with population parameters from cor_matrix_bigfive

summary_stats_mean

```
### Exercise 4.1

```{r 100 planets}
n_colonies <- 100
n_colonist <- 100
trait_names <- c("EX", "ES", "AG", "CO", "OP")
means_bigfive <- c(EX = 50, ES = 80, AG = 60, CO = 75, OP = 55)
sds_bigfive <- c(EX = 5, ES = 5, AG = 5, CO = 5, OP = 5)

cor_matrix_bigfive <- matrix(
  c(
    1.0000, 0.2599, 0.1972, 0.1860, 0.2949,
    0.2599, 1.0000, 0.1576, 0.2306, 0.0720,
    0.1972, 0.1576, 1.0000, 0.2866, 0.1951,
    0.1860, 0.2306, 0.2866, 1.0000, 0.1574,
    0.2949, 0.0720, 0.1951, 0.1574, 1.0000
  ),
  nrow = 5, byrow = TRUE,
  dimnames = list(trait_names, trait_names)
)

cov_matrix_bigfive <- cor2cov(cor_matrix_bigfive, sds_bigfive)

set.seed(1254)
## Use replicate to generate colonist data for 100 colonies
all_colonies <- replicate(n_colonies, {
  bigfive_data <- mvrnorm(n = n_colonist, mu = means_bigfive, Sigma = cov_matrix_bigfive)
  bigfive_df <- as.data.frame(bigfive_data)
  colnames(bigfive_df) <- trait_names
  bigfive_df
}, simplify = FALSE)

## Add colony ID and combine into one data frame
colonies_df <- bind_rows(
  lapply(1:n_colonies, function(i) {
    all_colonies[[i]] %>%
      mutate(colony_id = i, colonist_id = 1:n_colonist)
  })
)

## mean, sd, and correlation for each colony
colony_stats <- colonies_df %>%
  group_by(colony_id) %>%
  summarize(
    EX_mean = mean(EX),
    EX_sd = sd(EX),
    EX_OP_cor = cor(EX, OP),
    .groups = "drop"
  )

## View the result
colony_stats

##plot
ggplot(colony_stats, aes(x = EX_OP_cor)) +
  geom_histogram(binwidth = 0.02, fill = "lightblue", color = "pink") +
  labs(
    title = "Distribution of EX–OP Correlation Across Colonies",
    x = "EX–OP Correlation",
    y = "Frequency"
  )

```
based on the table, the correlations between extraversion and openness vary a little across the 100 planets, and mainly ranging from 0.08 to 0.5. It seems like most correlations centers on .29, which is the value we have set for.
The relationships only seemed relatively consistent since this is a normal distribution, but there are correlations that is relatively far from .29. I think this sample size is pretty stable, but a larger one, such as 1000, can be more stable. I am not sure how to judge this so I just gave a rough number.

### Exercise 4.2

```{r}

## I asked GPT if there is an easier way to plot all the graph together and look at the data. This is the instructions. I think it looks pretty decent. 
##install.packages("gridExtra") 
library(gridExtra)
library(ggplot2)
# Population parameters
pop_mean_ex <- 50
pop_sd_ex <- 5
pop_cor_ex_op <- 0.2949

# Plot 1: EX Mean
p1 <- ggplot(colony_stats, aes(x = EX_mean)) +
  geom_histogram(binwidth = 0.25, fill = "skyblue", color = "white") +
  geom_vline(xintercept = pop_mean_ex, linetype = "dashed", color = "red", size = 1) +
  annotate("text", x = pop_mean_ex + 0.3, y = max(table(cut(colony_stats$EX_mean, breaks=30))), 
           label = "Population Mean = 50", hjust = 0, color = "red") +
  labs(title = "Distribution of Extraversion Means",
       x = "Mean of Extraversion", y = "Count") +
  theme_minimal()

# Plot 2: EX SD
p2 <- ggplot(colony_stats, aes(x = EX_sd)) +
  geom_histogram(binwidth = 0.1, fill = "lightgreen", color = "white") +
  geom_vline(xintercept = pop_sd_ex, linetype = "dashed", color = "red", size = 1) +
  annotate("text", x = pop_sd_ex + 0.2, y = max(table(cut(colony_stats$EX_sd, breaks=30))), 
           label = "Population SD = 5", hjust = 0, color = "red") +
  labs(title = "Distribution of Extraversion SDs",
       x = "Standard Deviation of Extraversion", y = "Count") +
  theme_minimal()

# Plot 3: Corr(EX, OP)
p3 <- ggplot(colony_stats, aes(x = EX_OP_cor)) +
  geom_histogram(binwidth = 0.02, fill = "lightcoral", color = "white") +
  geom_vline(xintercept = pop_cor_ex_op, linetype = "dashed", color = "red", size = 1) +
  annotate("text", x = pop_cor_ex_op + 0.02, y = max(table(cut(colony_stats$EX_OP_cor, breaks=30))), 
           label = "Population r = 0.29", hjust = 0, color = "red") +
  labs(title = "Distribution of EX–OP Correlations",
       x = "Correlation Between EX and OP", y = "Count") +
  theme_minimal()

# Arrange plots side-by-side
grid.arrange(p1, p2, p3, ncol = 1)
```

### Overall 
In general, I also really enjoyed this lab, especially exercise 4 when we would replicate for 100 planets. I think it is really useful when we are trying to learn and understand simulation. I think for 4.2., we can add some questions to promote a deeper thinking about the graph, such as: What does these graphs tell you about the reliability of trait estimates drawn from a single population? I am not sure if this question fits the purpose of this lab, but I think adding an interpretation question would be interesting. For Exercise 4.1., I think we can also plot two graphs with one n= 50 and one n = 1000 to answer your question of: How large of a sample size would you need to get a stable estimate of the correlation between extraversion and openness?

